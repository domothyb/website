---
layout: post
title: Anatomy of a Blob
---

*Warning: This article gets pretty mathy, pretty fast. It is meant to be the more technical counter-part to a more general article about blobspace coming soon.*

In the next network upgrade, Ethereum is slated to roll out [EIP-4844](https://eips.ethereum.org/EIPS/eip-4844) (nicknamed *proto-danksharding*) to drastically reduce the costs of rollups when it comes to committing data on Ethereum. 

The main ingredient of this EIP is what's known as a *blob*. In this article we will look at what a blob is, how it's used, and we will dissect one to see how all the cryptography works under the hood.

## What a blob is

In a nutshell, a blob is simply a new piece that gets attached to Ethereum transactions, which can hold a certain amount of arbitrary data. The main goal is to give rollups a place to commit data from batched transactions that's cheap enough to reduce their operating costs and, in turn, drastically lower [L2 fees](https://l2fees.info).

EIP-4844 introduces the concept of "blobspace", the counterpart to the existing "blockspace" that we know and love:

* **Blockspace**: Where *execution* happens, e.g. the result of an ETH transfer or a trade on Uniswap 
* **Blobspace**: Where *data* is committed and made available to all who want it, without directly affecting Ethereum's state

This explicit separation between *execution* and *data* is crucial when it comes to understanding the whole point of Ethereum's [**rollup-centric roadmap**](https://ethereum-magicians.org/t/a-rollup-centric-ethereum-roadmap/4698): it's much easier to scale *blobspace* than it is to scale *blockspace*. For this reason, Layer 1 execution is expected to remain congested for the foreseeable future, which means that Layer 1 fees will remain high.

The job of rollups is then to take this scalable blobspace offered by Layer 1, and convert it into scalable Layer 2 execution. The magic happens – speaking very broadly – by compressing the results of many L2 transactions into a single L1 blob; what was L2 execution became L1 data! 

## How blobs are made

Under the hood, constructing a blob involves a lot of complicated math that I hope to demystify shortly. As with a lot of nifty cryptographic constructions, it all boils down to polynomials. Please bear with me for this section, trust me it has to be this way.

As per EIP-4844 specs, a blob can contain up to ~125 kilobytes of arbitrary data. With your data in hand, here's the step-by-step recipe to go craft a blob:

* Take your data, and break it up into 4096 chunks of 32 bytes each[^1]. Pad with 0s if you don't have enough data to fill 4096 chunks.
* Treat each chunk as a number[^2], and then use those numbers to compute a polynomial equation $P(x)$ such that $P(\omega^i)$ is equal to the $i$-th chunk of data
  * $\omega$ is a constant that satisfies $\omega^{4096} = 1$. Interpolating such that $P(i)$ equals the $i$-th chunk directly would work too, but using $\omega$ allows fancy optimization techniques to speed up the computation.
* Take $P(x)$ and evaluate it at a very specific top-secret point $s$ that is unknown by everyone in the world (more on this later)
  * This value $P(s)$ is known as the *commitment* of the blob, and effectively acts as a hash function: Tweaking any part in the blob's data completely changes the commitment, going from data to a commitment is easy, but the other way around is impossible, and you can't find two different blobs that have the same commitment.
* The commitment then goes through a last round of processing to become a *versioned commitment*: Start with a version byte `0x01` followed by the last 31 bytes of the SHA256 hash of $P(s)$.
    * This is done to make sure the commitment is 32 bytes long like the EVM likes, and to keep the door open for future versions of commitments that rely on a different scheme (e.g. for quantum-resistant purposes)
    * The EVM has no access to the actual data contained in the blob, but it will have access to this final versioned commitment, which will allow all sorts of ZK magic by using even more polynomial magic, as we will see later.

### Dissecting a blob

Don't get discouraged just yet, the recipe I laid out above is still fairly complicated, so I think it's useful to go through a scaled down example by hand, in order to really grasp what actually happens under the hood as understanding it all is a key component for what will come next.

Our case study will involve a much smaller blob, at just 16 bytes long which will be split in 8 chunks of 2 bytes. The data it will contain is `I'm a tiny blob!` — we will mercilessly dissect that blob for science.

But first, we need to decide on a *field modulus* $p$ that will determine in which *finite field* all the math will be happening. This is fancy terminology to say that instead of regular numbers that can blow up to infinity or get divided into ugly fractions, all the math we do will be with clean integers between 0 and $p-1$, under [modular arithmetic](https://en.wikipedia.org/wiki/Modular_arithmetic). All the usual properties of addition and multiplication hold, with the exception that any time above $p-1$ or below 0 will wrap around to stay within the range.

For technical reasons that I won't get into, $p$ has to be a prime number. Let's pick $p=65521$ for no particular reason. This prime is just under $2^{16}$, so every field element can be encoded within 16 bits or 2 bytes. Unfortunately we can't encode the numbers $65521$ through $65535$ since under modular arithmetic, they just get wrapped around to $0$ through $14$. This means our field elements can actually only contain just under 2 bytes of information[^3], and so our final blobs will themselves be able to hold slightly less than 16 bytes of information – this is an unfortunate quirk we'll just have to live with.

Since we'll be splitting our data into 8 chunks of 2 bytes, we have to find an element $\omega$ such that $\omega^{8} = 1$. The fancy name for $\omega$ is "an primitive 8th root of unity". Just by knowing what it's called, finding one becomes [very easy with the right tool](/images/blob/find-roots-of-unity.png). We get that $\omega = 7669$ is a suitable candidate.

We take note of the first 8 powers of $\omega$ mod $p$ along with an additional one to make sure it does in fact behave like a primitive root of unity with the desired property that $\omega^{8} = 1$:

* $\omega^0 = 1$
* $\omega^1 = 7669$
* $\omega^2 = 41224$
* $\omega^3 = 8031$
* $\omega^4 = 65520$
* $\omega^5 = 57852$
* $\omega^6 = 24297$
* $\omega^7 = 57490$
* $\omega^8 = 1$

Alright, we finally have everything to start turning our data into a tiny blob. Picking ASCII as a natural character encoding, our data `I'm a tiny blob!` is first encoded into the following series of 16 numbers:

`73 39 109 32 97 32 116 105 110 121 32 98 108 111 98 33`

We need 8 chunks of 2 numbers, so another natural choice here is to turn the pair of numbers $(a,b)$ into the number $256×a + b$, e.g the first pair (73, 39) becomes 18,727, the second pair becomes 27,936, and so on. We get our list of 8 numbers:

`18727, 27936, 24864, 29801, 28281, 8290, 27759, 25121`

Luckily enough, none of those numbers go "out of bound" since they are all below $65521$. We now need to find a polynomial $P(x)$ such that $P(\omega^i)$ is equal to the $i$-th chunk. This means we want $P(1) = 18727$, $P(7669) = 27936$, etc. [One FFT](https://github.com/ethereum/research/blob/master/mimc_stark/fft.py#L31) later and we get:

$$P(x) = 64798 + 44990*x + 15726*x^2 + 10416*x^3 + 42011*x^4 + 9197*x^5 + 32011*x^6 + 61662*x^7$$

As usual, it's worth doing [a sanity check](/images/blob/sanity-check.png) to make sure we didn't mess up anywhere. 

Another way to view $P(x)$ is that its coefficients are 8 new numbers that not only "encode" our original 8 numbers (which themselves encode the original data), but also "extend" them to the entire range of numbers between $0$ and $65521$. A first hint as to why this is useful is to note that if you're given the evaluation of $P$ at any 8 random points, you can interpolate the polynomial and get the data back by evaluating for all powers of $\omega$.

Now we need to evaluate $P$ at some random point $s$ that nobody in the world knows about. Glossing over some more math which I'll come back shortly, just know we're only given an "encrypted" version of $s$, denoted as $[s]$, which doesn't tell us anything about $s$. Through useful mathematical properties, we can still compute $P([s])$ and it'll be equal to $[P(s)]$

The last step is a less interesting one, the final commitment as seen by the EVM would simply be `0x01` followed by the last 31 bytes of `SHA256([P(37564)])`.

Let's recap what we just did in comparison to EIP-4844:

* Instead of holding ~128 kilobytes, our blob only holds ~16 bytes 
* Instead of splitting the data up into 4096 chunks, we only had 8 chunks
* Instead of the prime $p$ being ~32 bytes long[^4], ours was just 2 byte long
* Instead of having $\omega$ be a 4096th root of unity, our $\omega$ was an 8th root of unity

### Summoning the secret $s$

It's worth taking a quick detour to get an understanding about how we can actually get this secret value $s$ that absolutely nobody must know.

It all has to do with [elliptic curve scalar multiplication](https://en.wikipedia.org/wiki/Elliptic_curve_point_multiplication), which is a pretty complicated branch of math if you're not already familiar with it. For our purposes however, all you really need to know is that these fancy elliptic curves allow us to craft a function that "encrypts" numbers in a one-way fashion.

Reusing the same notation from earlier, $[x]$ is the encrypted version of $x$. Aside from being a one-way function (easy to go $x \rightarrow [x]$, impossible to go $[x] \rightarrow x$) there are two other important mathematical properties we care about:

1. $n\cdot[x] = [n\cdot x]$
2. $[x] + [y] = [x+y]$

These properties are pretty powerful, because they let us do computations without having to know the values we're using in the computation. Namely, if you're given the encrypted powers of $s$, you can evaluate the encrypted version of $P(s)$ without ever knowing the actual value of $s$:

$P([s]) = c_0[s^0] + c_1[s^1] + c_2[s^2] +\ ... +\ c_n[s^n]$  
$= [c_0\cdot1] + [c_1\cdot s] + [c_2\cdot s^2]\ +\ ...\ +\ [c_n\cdot s^n]$  
$= [c_0\cdot1 + c_1\cdot s + c_2\cdot s^2\ +\ ...\ + c_n\cdot s^n]$  
$= [P(s)]$

The only problem remaining is that whoever gave you the encrypted powers of $s$ necessarily had to know the unencrypted value of $s$. You have to trust that they deleted their copy of $s$ after they were done, or that their random generator worked as it should, or that their computer wasn't being spied on, etc. The solution is to have many participants as possible each computing their own secrets, and then mixing all the secrets together sequentially:

* Alice generates a random number $a$ and publishes $[a]$, $[a^2]$, $[a^3]$, ...
* Bob generates a random number $b$ and – without knowing $a$ – publishes $b[a] = [ba]$, $b^2[a^2] = [b^2\cdot a^2] = [(ba)^2]$, $b^3[a^3] = [(ba)^3]$, ...
* Charlie generates a random number $c$ and – without knowing either $a$ or $b$ – publishes $c[ba] = [cba]$, $c^2[(ba)^2] = [(cba)^2]$, $c^3[(ba)^3] = [(cba)^3]$, ...

Now the secret $s$ is simply $cba$, and the only way to get the unencrypted secret is if all three of them kept their random number, and agree to collude together to multiply them together. Or in other words, as long as just one person participated honestly and threw away their secret, the resulting final secret $s$ is safe and nobody in the world knows what it is.

Scale this little secret-summoning ceremony to over a hundred thousand participants, and the probability that all of them collude is effectively 0%. Thankfully, this is what [Ethereum's own ceremony](https://ceremony.ethereum.org/) looked like!

By the way, the proper term for this is a KZG trusted setup, you can read more about it from Vitalik's blog post [here](https://vitalik.ca/general/2022/03/14/trustedsetup.html)

## Why blobs are like this

If you made it this far, you might be wondering about why we went through so much effort: if the goal is just to commit to some data, then `SHA256(data)` achieves the exact same thing. But I assure you, there are several good reasons for going down the polynomial path:

**Erasure coding**: By converting our data into a polynomial $P(x)$, we can evaluate it at any point beyond just the original 8 ones, so adding redundancy is trivial: double the data by evaluating $P$ at 8 more points, and then up to 8 of the 16 evaluations can be lost and we can still recover the original data by interpolating the original polynomial.

**Evaluation proofs**: If I know the polynomial, I can evaluate it at any point $x$ and send you the evaluation $P(x)$ along with a short proof. Then you don't have to know the whole polynomial to verify that I gave you a correct evaluation, you just need the short commitment and the (equally short) proof and do a very quick bit of computation.

**zkRollup magic**: While smart contract don't have direct access to the data contained in blobs, they do have access to the commitment, along with a way to verify evaluation proofs. The design space for how this can be used is wide open, but the most basic usage for zkRollups would be to commit a whole bunch of rollup transactions and their resulting effect on the rollup's state, and then the (hopefully immutable) smart contract can easily and cheaply[^5] check a single evaluation proof to be convinced of the validity of all the transactions, and proceed with all the bridging of funds and stuff.

**Scaling data availability**: When the data is structured into these clever polynomials, you only need to query a few random samples, check a few short proofs, and you'll be convinced that the entire data is available for anyone who wants it, without having to download the entire data yourself. Just a few random samples suffice, *regardless of how much data there actually is*. To really drive the point home, this means a fork of Ethereum where some data has been withheld will be considered invalid to the same degree as a fork where a transaction tries to steal funds without the proper private keys. In both cases, nodes should and will reject that fork and keep following the fork where everything checks out. 

This nifty sampling is the core of how polynomials will help scale Ethereum, so it's worth exploring a bit more: consider a large amount of data being transferred over the network. Without this trick, every node has to download every single byte of data in every blob to make sure everything checks out. This doesn't scale well, as increasing the sizes of blobs also increases the bandwidth and storage requirements for every single node. 

Instead, with polynomial magic we can safely distribute the load across validators. With erasure coding, we have the following statement: *If at least 50% of the (extended) data is available, then **all** of the data is available.* With that in mind, you can now simply ask for a bunch of random samples and check the evaluation proofs you get in return.

To view this in mathematical terms, if less than 50% of the extended data has actually been published, then the odds that a single random sample succeeds is less than 50% (an attacker intentionally trying to hide data from the network can't predict which random number you'll ask for) if you ask for 2 random samples and both succeed, the probability that you just happened to ask for two points from the less than 50% of available data is less than 25%, and so on.

After just 30 random samples, you can be sure that all of them succeeding wasn't just dumb luck, as the odds of such dumb luck happening would be less that $\frac{1}{2^{30}}$, which is about one in a billion.

With this nifty trick, your node was able to check for data availability without suffering the burden of downloading the entire data. Even more importantly, regardless of how much data is actually circulating on the network, these 30 random samples will suffice and remain small and easily verifiable. 





[^1]: It's actually slightly less than that at ~31.86 bytes, but that's not important right now.
[^2]: *Field element* is a better term than *number*, but that's also not important right now.
[^3]: The same thing happens at a bigger scale for actual blobs, hence why as footnote 1 says, each field element holds slightly less than 32 bytes of information, so the actual maximum blob size is closer to 127.4 kilobytes of information instead of the 128 kilobytes you'd expect if every element was a clean 32 bytes
[^4]: If you must know, `p = 52435875175126190479447740508185965837690552500527637822603658699938581184513`
[^5]: Checking an evaluation proof will cost 50,000 gas, as per EIP-4844 specs.